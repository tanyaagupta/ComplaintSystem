# -*- coding: utf-8 -*-
"""vihaan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16FrstIQ6-Z6ZENNmgDoxgFBAT20Xywvl
"""

import numpy as np
import pandas as pd
import os

import string
import plotly.express as px
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import pandas as pd
import string

!pip install lemminflect

os.environ["KAGGLE_USERNAME"] = "pallika"
os.environ["KAGGLE_KEY"] = "3432bc74a9011c791671dcfdabcab6cc"

!kaggle datasets download -d omarsobhy14/university-students-complaints-and-reports

!unzip /content/university-students-complaints-and-reports.zip

df = pd.read_csv('/content/Datasetprojpowerbi.csv')

df.sample(10)

df['Genre'].unique()

dfcount=df['Genre'].value_counts()
dfcount

#DATA PRE-PROCESSING

df.drop(['Age','Gpa','Year','Count','Gender'], axis=1, inplace=True)
df

allSentences=[]
for complain in df['Reports']:
  #This line removes all punctuation marks from the current sentence and replace with empty strings.
  complain = complain.translate(str.maketrans('', '', string.punctuation))
  # This line removes double quotation marks (") from the current sentence by replacing them with an empty string
  complain=complain.replace('"','')
  allSentences.append(complain)
df['Reports']=pd.DataFrame(allSentences)
df

#TOKENIZATION



i=0
tokens = []
for review in df['Reports']:
  word=word_tokenize(str(review))
  tokens.append(word)
print(tokens)

token_df = pd.DataFrame({'Tokens': tokens})
print(token_df)

df['Tokenised_words']=tokens
df

#STOP WORDS

print(stopwords.words('english'))

stpwrd = nltk.corpus.stopwords.words('english')

all=[]
for lis in df['Tokenised_words']:
  for wrd in lis:
    if wrd in stpwrd:
      lis.remove(wrd)
  lis=" ".join(lis)
  all.append(lis)

token_df2 = pd.DataFrame({'Tokens': all})
df['updated']=token_df2
df[['Reports','updated']]

"""#Lemmatization & Stemming

* Stemming -- removing characters from a word such that it represents core meaning of the word even though the word is not valid,eg: running --> run
* Lemmatization: reduce words to their base or dictionary form, which is known as the lemma.
Unlike stemming, lemmatization considers the context and meaning of the word before reducing it to its core meaning, tends to produce valid words
"""

#spacy: sentence segmentation, lemmatization, word vectors, text classification
#lemminflect- extends functionality of spacy,generating various forms of a word, such as plurals, verb conjugations, and comparative/superlative adjectives.

import spacy
import lemminflect
nlp = spacy.load('en_core_web_sm')

# PorterStemmer algorithm - stemming algorithm
# The WordNetLemmatizer in NLTK uses WordNet to perform lemmatization.

from nltk.stem import PorterStemmer
ps=PorterStemmer()
from nltk.stem import WordNetLemmatizer
lem=WordNetLemmatizer()

# nltk.download('wordnet'): WordNet is a lexical database that provides semantic relationships between words and concepts
# used in tasks like lemmatization, synonym detection, and word sense disambiguation.

import nltk
nltk.download('wordnet')

words=word_tokenize(df['updated'].iloc[0])
for w in words:
    print(w, " : ", ps.stem(w))

lemm=[]
words=[]
Docm = nlp(df['Reports'].iloc[0])
for tkn in Docm:
    print(tkn," : ",tkn._.lemma())

stemmToken=[]
for i in range(0,len(df)):
  empty=[]
  emptystem=[]
  words=word_tokenize(df['updated'].iloc[i])
  for w in words:
    emptystem.append(ps.stem(w))
  stemmToken.append(emptystem)

df['stemmCleaned']=stemmToken
df[['Reports','updated','stemmCleaned']]

df['stemmCleaned'] = df['stemmCleaned'].apply(' '.join)
df

lemm=[]
for i in range(len(df)):
  words=[]
  doc = nlp(df['Reports'].iloc[i])
  for token in doc:
    if str(token) not in stpwrd:
      words.append(token._.lemma())
  lemm.append((" ".join(words)))

dflemm=pd.DataFrame(lemm)
df['Lemminfed']=dflemm
df

"""#Vectorization
* Vectorization in the context of natural language processing (NLP) refers to the process of converting text data into numerical vectors that can be used as input to machine learning algorithms.

"""

df['Lemminfed'][0]

from sklearn.feature_extraction.text import TfidfVectorizer
corpus = df['Lemminfed']
vectorizer = TfidfVectorizer()
T = vectorizer.fit_transform(corpus)
feature_names=vectorizer.get_feature_names_out()

T[0]

corpus = df['stemmCleaned']
vectorizer = TfidfVectorizer()
S = vectorizer.fit_transform(corpus)
feature_names_S=vectorizer.get_feature_names_out()
S

import pickle

with open('vectorizer.pkl', 'wb') as f:
  pickle.dump(vectorizer, f)

df_tfidfvectS = pd.DataFrame(data = S.toarray(),columns = feature_names_S)
df_tfidfvectS

"""#Clustering Using Kmeans"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=12, init='k-means++', random_state= 42)
y_predict_S= kmeans.fit_predict(S)
df['y_predict_stemm'] = pd.DataFrame(data = y_predict_S)



kmeans = KMeans(n_clusters=12, init='k-means++', random_state= 42)
y_predict_T= kmeans.fit_predict(T)
df['y_predict_inflemm'] = pd.DataFrame(data = y_predict_T)

df['count']=1

import plotly.express as px

fig = px.bar(df, x="y_predict_stemm", y="count", color="Genre", title="Long-Form Input")
fig.show()

from collections import Counter

cluster = df.groupby('y_predict_stemm')

wordCount = {}
for name, group in cluster:
    words = ' '.join(group['Lemminfed']).split()
    wordCount[name] = Counter(words)

topWord = {}
for cls, counter in wordCount.items():
    topWord[cls] = [word for word, count in counter.most_common(15)]
for cls, words in topWord.items():
    print(f"Cluster {cls}: {', '.join(words)}")

fig = px.bar(df, x="y_predict_inflemm", y="count", color="Genre", title="Long-Form Input")
fig.show()

from collections import Counter


cluster = df.groupby('y_predict_inflemm')

wordCount = {}
for name, group in cluster:
    words = ' '.join(group['Lemminfed']).split()
    wordCount[name] = Counter(words)

topWord = {}
for cls, counter in wordCount.items():
    topWord[cluster] = [word for word, count in counter.most_common(15)]
for cls, words in topWord.items():
    print(f"Cluster {cls}: {', '.join(words)}")

"""##Not the optimim algorithm for this dataset as there are clusters that are not that clean
But what is the optimum number of clusters for this dataset?
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

distorts = []
K = range(1,20)
for k in K:
    kmean = KMeans(n_clusters=k,random_state=7)
    kmean.fit(T)
    distorts.append(kmean.inertia_)

plt.figure(figsize=(20,5))
plt.plot(K, distorts, '-',color='g')
plt.xlabel('k values')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

from sklearn.metrics import silhouette_score
from yellowbrick.cluster import KElbowVisualizer

K = range(2,20)
silhouette = []
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(T)
    preds = kmeanModel.predict(T)
    silhouette.append(silhouette_score(T, preds))

plt.figure(figsize=(20,5))
plt.plot(K, silhouette, '-',color='g')
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score of each k values')
plt.show()

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
PCA = pca.fit(T.toarray())
X_pca = pca.transform(T.toarray())

kmeans = KMeans()
visualizer = KElbowVisualizer(kmeans, k=(1,20),size=(1080, 500))

visualizer.fit(X_pca)
visualizer.show()

kmeanModel = KMeans(n_clusters=3)
kmeanModel.fit(X_pca)
pred_labels = kmeanModel.labels_;
pred_centers = kmeanModel.cluster_centers_
df_centers = pd.DataFrame(pred_centers, columns=['x', 'y'])
df_centers.head(1)

dfcl = pd.DataFrame(columns=['x','y','label'])
dfcl['x'] = X_pca[:,0]
dfcl['y'] = X_pca[:,1]
dfcl['label'] = kmeanModel.labels_
dfcl.head(1)

import plotly.express as px
fig = px.scatter(dfcl, x="x", y="y", color="label")
fig.show()

"""#Classification"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(T, df['Genre'], test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn import model_selection, datasets
from sklearn.tree import DecisionTreeClassifier
import joblib
import pickle
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

rf.fit(X_train, y_train)

filename = "final_model_classification.joblib"
joblib.dump(rf, filename)

with open('model2.pkl', 'wb') as f:
    pickle.dump(rf, f)

with open('model2.pkl', 'rb') as f:
    model2 = pickle.load(f)

model = joblib.load("final_model_classification.joblib")

X_test[0]

# input_str = 'The campus is e asdklfjlak dflkaj dslkfjslk dfjlksj dfxtra dirty.'
# words=[]
# doc = nlp(input_str)
# for token in doc:
#   if str(token) not in stpwrd:
#     words.append(token.lemma_)
# corpus = (" ".join(words))
# T = vectorizer.transform([corpus])
# model2.predict(T)

T

model.predict(X_test[0])

from sklearn.metrics import accuracy_score

y_pred = rf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("classification:", y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# corpus = [
#     'food is terrible in the cantines',
#      'I am struggling this semester and the exams are too difficult',
#      'I got rejected in the sports basketball team however i am pretty sure i am the best sports player out there',
#      'Bus leaves me each day and i miss it out,is there a chance we can delay the pickup time'
#  ]
# new_text_tfidf = vectorizer.transform(corpus)
# predicted_label = svm_model.predict(new_text_tfidf)

# print("Predicted label:", predicted_label)

from sklearn.svm import SVC
svm_model = SVC(kernel='linear', C=1)
svm_model.fit(X_train, y_train)

y_pred_1 = svm_model.predict(X_test)
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred_1)

print("Accuracy:",accuracy)

